# 梯度下降算法学习笔记

Author:王琛锡

time：2021.11.3

## 首先要解决的数学问题

- 全微分的定义
- 方向导数的定义及其几何意义
- 梯度的定义及其几何意义

### 全微分的定义

为了方便，手写公式，插入图片

![image-20211103180631219](/Users/chenxiwang/Library/Application Support/typora-user-images/image-20211103180631219.png)

### 方向导数及梯度

![image-20211103164930509](/Users/chenxiwang/Library/Application Support/typora-user-images/image-20211103164930509.png)

![image-20211103165003322](/Users/chenxiwang/Library/Application Support/typora-user-images/image-20211103165003322.png)

### 梯度下降求解线性回归

##### 第一张图片中的几个问题

![image-20211103165502084](/Users/chenxiwang/Library/Application Support/typora-user-images/image-20211103165502084.png)

对于上图红框中的内容主要的疑问为：

- 红框中":= "是什么意思？
- 为什么$\theta$ 的更新用的是减号而不是加号？

###### := 的意思

:= 应该是来自于吴恩达机器学习视频中的符号表示，根据吴老师的介绍，:= 意思就是右边的$\theta$减去$\alpha$乘以$\theta$的偏导数等于左边的$\theta$，这是一个对$\theta$进行更新的过程。

这儿对于我们熟悉变成的人来说可能有点疑惑，平时我们更新一个变量本身不就是 a = a + b或者

a += b这么去更新的吗？为什么这儿更新会变成 := ？

这个地方我个人更愿意理解为 := 是一种严格的数学语言，比如我们在数学中就不能说 4 = 4 + 1，这样是错误的，只不过这个地方把4换成了变量而已。其实我们在具体的Python或者其他语言的编码当中，此处更新$\theta$当然还是用的$\theta$ = $\theta$ - 它的偏导。

###### 为什么更新用的是减号

这个可能需要联系梯度的定义，数学中这么定义梯度：

空间中某点的梯度就是该点方向导数最大的方向。

注意：梯度是向量，向量是有方向的，这个方向是导数最大的方向，即函数值增大最快的方向，而我们的梯度下降，是在求这个误差函数的函数值能最快变得最小的那个方向，换而言之，这个方向是与梯度向量方向在一条直线上但完全相反的方向，就像：

​															$\uparrow$ 梯度方向，函数值变大最快的方向
​															$\downarrow$ 梯度下降方向，函数值变小最快的方向

我们知道假设对于一个 $Z=(x,y)$的二元函数来说，它在$(x,y)$点的梯度就是向量$(\frac{\partial Z}{\partial x},\frac{\partial Z}{\partial y})$,此时要沿着梯度的反方向变化，这就是“—”而不是“+”的原因，当然是$Z=(x,y)$点的横纵坐标同比例的（这也是乘$\alpha$再减的原因）加上梯度相反方向向量，想像一个空间点沿着梯度相反方向移动，具体移动多少，看步长$\alpha·\frac{\partial Z}{\partial x}$和$\alpha·\frac{\partial Z}{\partial y}$的大小。

画个图：

![image-20211103174931435](/Users/chenxiwang/Library/Application Support/typora-user-images/image-20211103174931435.png)

上图反应的是沿梯度方向变化，及增大，对比可以了解沿梯度反方向下降的过程。

##### 第二张图片中的一个问题

![image-20211103175402466](/Users/chenxiwang/Library/Application Support/typora-user-images/image-20211103175402466.png)

###### 上述两个框中的表达式为什么不一样？

这个其实比较好回答，首先要弄清楚我们这个函数求偏导的过程中谁是未知数，谁是已知数，我们是在对谁求偏导？

在上述的表达式中，$x，y$其实是我们样本中已知的数据，未知数是表达式里的众多$\theta$，对于有$x$这个系数的未知数$\theta$，对$\theta$求完偏导之后当然还应该乘以这个系数$x$,而对于没有系数的未知数$\theta$，求完导后$\theta$成了1，当然就没有$x$了。